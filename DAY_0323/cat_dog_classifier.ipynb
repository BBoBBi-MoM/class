{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Model\n",
    "import keras.layers as layers\n",
    "from keras.datasets import cifar10\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping,LearningRateScheduler\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import PIL\n",
    "\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow: 2.6.0\n",
      "keras: 2.6.0\n"
     ]
    }
   ],
   "source": [
    "print('tensorflow:',tf.__version__)\n",
    "print('keras:',keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 101000 images belonging to 101 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   validation_split=0.2)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(r'C:\\Users\\Administrator\\Desktop\\Dataset\\food101',\n",
    "                                                 target_size = (224,224),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3157"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG11(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_layer1 = tf.keras.Sequential([\n",
    "            layers.Conv2D(filters=64,kernel_size=3,strides=1,padding=\"same\"),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.ReLU()\n",
    "        ])\n",
    "        self.sub_sampling_layer1 = layers.MaxPooling2D(pool_size=(2, 2),strides=2)\n",
    "        \n",
    "        self.conv_layer2 = tf.keras.Sequential([\n",
    "            layers.Conv2D(filters=128,kernel_size=3,strides=1,padding=\"same\"),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.ReLU()\n",
    "        ])\n",
    "        self.sub_sampling_layer2 = layers.MaxPooling2D(pool_size=(2, 2),strides=2)\n",
    "        \n",
    "\n",
    "        self.conv_layer3 = tf.keras.Sequential([\n",
    "            layers.Conv2D(filters=256,kernel_size=3,strides=1,padding=\"same\"),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.ReLU()\n",
    "        ])\n",
    "        self.conv_layer4 = tf.keras.Sequential([\n",
    "            layers.Conv2D(filters=256,kernel_size=3,strides=1,padding=\"same\"),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.ReLU()\n",
    "        ])\n",
    "        self.sub_sampling_layer3 = layers.MaxPooling2D(pool_size=(2, 2),strides=2)\n",
    "\n",
    "\n",
    "        self.conv_layer5 = tf.keras.Sequential([\n",
    "            layers.Conv2D(filters=512,kernel_size=3,strides=1,padding=\"same\"),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.ReLU()\n",
    "        ])\n",
    "        self.conv_layer6 = tf.keras.Sequential([\n",
    "            layers.Conv2D(filters=512,kernel_size=3,strides=1,padding=\"same\"),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.ReLU()\n",
    "        ])\n",
    "        self.sub_sampling_layer4 = layers.MaxPooling2D(pool_size=(2, 2),strides=2)\n",
    "\n",
    "        self.conv_layer7 = tf.keras.Sequential([\n",
    "            layers.Conv2D(filters=512,kernel_size=3,strides=1,padding=\"same\"),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.ReLU()\n",
    "        ])\n",
    "        self.conv_layer8 = tf.keras.Sequential([\n",
    "            layers.Conv2D(filters=512,kernel_size=3,strides=1,padding=\"same\"),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.ReLU()\n",
    "        ])\n",
    "        self.sub_sampling_layer5 = layers.MaxPooling2D(pool_size=(2, 2),strides=2)\n",
    "        \n",
    "        self.flatten = layers.Flatten() \n",
    "\n",
    "        self.fc_layer1 = tf.keras.Sequential([\n",
    "        layers.Dense(4096),\n",
    "        layers.ReLU(),\n",
    "        layers.Dropout(0.5)\n",
    "        ])\n",
    "        self.fc_layer2 = tf.keras.Sequential([\n",
    "        layers.Dense(4096),\n",
    "        layers.ReLU(),\n",
    "        layers.Dropout(0.5)\n",
    "        ])\n",
    "        self.output_layer = tf.keras.Sequential([\n",
    "        layers.Dense(101),\n",
    "        layers.Softmax()\n",
    "        ])\n",
    "\n",
    "    def call(self,x):\n",
    "        x = self.conv_layer1(x)\n",
    "        x = self.sub_sampling_layer1(x)\n",
    "        x = self.conv_layer2(x)\n",
    "        x = self.sub_sampling_layer2(x)\n",
    "        x = self.conv_layer3(x)\n",
    "        x = self.conv_layer4(x)\n",
    "        x = self.sub_sampling_layer3(x)\n",
    "        x = self.conv_layer5(x)\n",
    "        x = self.conv_layer6(x)\n",
    "        x = self.sub_sampling_layer4(x)\n",
    "        x = self.conv_layer7(x)\n",
    "        x = self.conv_layer8(x)\n",
    "        x = self.sub_sampling_layer5(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc_layer1(x)\n",
    "        x = self.fc_layer2(x)\n",
    "        x = self.output_layer(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vg_g11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential (Sequential)      (None, 224, 224, 64)      2048      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) multiple                  0         \n",
      "_________________________________________________________________\n",
      "sequential_1 (Sequential)    (None, 112, 112, 128)     74368     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 multiple                  0         \n",
      "_________________________________________________________________\n",
      "sequential_2 (Sequential)    (None, 56, 56, 256)       296192    \n",
      "_________________________________________________________________\n",
      "sequential_3 (Sequential)    (None, 56, 56, 256)       591104    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 multiple                  0         \n",
      "_________________________________________________________________\n",
      "sequential_4 (Sequential)    (None, 28, 28, 512)       1182208   \n",
      "_________________________________________________________________\n",
      "sequential_5 (Sequential)    (None, 28, 28, 512)       2361856   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 multiple                  0         \n",
      "_________________________________________________________________\n",
      "sequential_6 (Sequential)    (None, 14, 14, 512)       2361856   \n",
      "_________________________________________________________________\n",
      "sequential_7 (Sequential)    (None, 14, 14, 512)       2361856   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 multiple                  0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "sequential_8 (Sequential)    (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "sequential_9 (Sequential)    (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "sequential_10 (Sequential)   (None, 101)               413797    \n",
      "=================================================================\n",
      "Total params: 129,191,141\n",
      "Trainable params: 129,185,637\n",
      "Non-trainable params: 5,504\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = VGG11()\n",
    "model.build(input_shape=(None,224,224,3))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "stop_callback = EarlyStopping(monitor='val_accuracy',patience=3)\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "  if epoch < 10:\n",
    "    return lr\n",
    "  else:\n",
    "    return lr * tf.math.exp(-0.1)\n",
    "\n",
    "lr_callbacks = LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = np.random.randn(32,224,224,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00940699, 0.00982758, 0.00998227, ..., 0.00967575, 0.01020838,\n",
       "        0.00949247],\n",
       "       [0.0094215 , 0.00983209, 0.0100017 , ..., 0.009766  , 0.01020879,\n",
       "        0.00955615],\n",
       "       [0.00945508, 0.00983882, 0.01002563, ..., 0.00972173, 0.01022084,\n",
       "        0.00951032],\n",
       "       ...,\n",
       "       [0.00947587, 0.00983904, 0.00995499, ..., 0.00971266, 0.01025793,\n",
       "        0.00958909],\n",
       "       [0.0094539 , 0.00982354, 0.00998302, ..., 0.00971254, 0.01017335,\n",
       "        0.0095186 ],\n",
       "       [0.00942012, 0.0098247 , 0.00998904, ..., 0.00971571, 0.01019591,\n",
       "        0.00954816]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(sample)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# `GPU인식이 안돼서 CIFAR-10으로 진행.`  \n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 데이터\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train,y_train),(X_test,y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype(float)/255\n",
    "X_test = X_test.astype(float)/255\n",
    "y_train = tf.keras.utils.to_categorical(y_train)\n",
    "y_test = tf.keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 10)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 모델 정의\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LenetLike(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_layer1 = tf.keras.Sequential([\n",
    "            layers.Conv2D(filters=6,kernel_size=5,strides=1,padding=\"same\"),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.ReLU()\n",
    "        ])\n",
    "        self.sub_sampling_layer1 = layers.MaxPooling2D(pool_size=(2, 2),strides=2)\n",
    "        \n",
    "        self.conv_layer2 = tf.keras.Sequential([\n",
    "            layers.Conv2D(filters=16,kernel_size=5,strides=1,padding=\"same\"),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.ReLU()\n",
    "        ])\n",
    "        self.sub_sampling_layer2 = layers.MaxPooling2D(pool_size=(2, 2),strides=2)\n",
    "        \n",
    "        self.flatten = layers.Flatten() \n",
    "\n",
    "        self.fc_layer1 = tf.keras.Sequential([\n",
    "        layers.Dense(512),\n",
    "        layers.ReLU(),\n",
    "        layers.Dropout(0.5)\n",
    "        ])\n",
    "        self.fc_layer2 = tf.keras.Sequential([\n",
    "        layers.Dense(512),\n",
    "        layers.ReLU(),\n",
    "        layers.Dropout(0.5)\n",
    "        ])\n",
    "        self.output_layer = tf.keras.Sequential([\n",
    "        layers.Dense(10),\n",
    "        layers.Softmax()\n",
    "        ])\n",
    "\n",
    "    def call(self,x):\n",
    "        x = self.conv_layer1(x)\n",
    "        x = self.sub_sampling_layer1(x)\n",
    "        x = self.conv_layer2(x)\n",
    "        x = self.sub_sampling_layer2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc_layer1(x)\n",
    "        x = self.fc_layer2(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"lenet_like_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_16 (Sequential)   (None, 32, 32, 6)         480       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 multiple                  0         \n",
      "_________________________________________________________________\n",
      "sequential_17 (Sequential)   (None, 16, 16, 16)        2480      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 multiple                  0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "sequential_18 (Sequential)   (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "sequential_19 (Sequential)   (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "sequential_20 (Sequential)   (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 795,546\n",
      "Trainable params: 795,502\n",
      "Non-trainable params: 44\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = LenetLike()\n",
    "model.build(input_shape=(None,32,32,3))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 10)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = np.random.rand(32,32,32,3)\n",
    "model.predict(sample).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "391/391 [==============================] - 27s 68ms/step - loss: 1.5080 - accuracy: 0.4519 - val_loss: 1.3963 - val_accuracy: 0.5052\n",
      "Epoch 2/100\n",
      "391/391 [==============================] - 26s 67ms/step - loss: 1.4999 - accuracy: 0.4559 - val_loss: 1.3900 - val_accuracy: 0.5080\n",
      "Epoch 3/100\n",
      "279/391 [====================>.........] - ETA: 7s - loss: 1.4924 - accuracy: 0.4574"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 16\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[39mreturn\u001b[39;00m lr \u001b[39m*\u001b[39m tf\u001b[39m.\u001b[39mmath\u001b[39m.\u001b[39mexp(\u001b[39m-\u001b[39m\u001b[39m0.1\u001b[39m)\n\u001b[0;32m     14\u001b[0m lr_callbacks \u001b[39m=\u001b[39m LearningRateScheduler(scheduler)\n\u001b[1;32m---> 16\u001b[0m model\u001b[39m.\u001b[39;49mfit(x\u001b[39m=\u001b[39;49mX_train,y\u001b[39m=\u001b[39;49my_train,\n\u001b[0;32m     17\u001b[0m           batch_size\u001b[39m=\u001b[39;49m\u001b[39m128\u001b[39;49m,\n\u001b[0;32m     18\u001b[0m           epochs\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m,\n\u001b[0;32m     19\u001b[0m           validation_data\u001b[39m=\u001b[39;49m(X_test,y_test),\n\u001b[0;32m     20\u001b[0m           callbacks\u001b[39m=\u001b[39;49m[stop_callback])\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py:1184\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1177\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1178\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   1179\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   1180\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[0;32m   1181\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m   1182\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m   1183\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1184\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1185\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1186\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:885\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    882\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    884\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 885\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    887\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    888\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:917\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    914\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    915\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    916\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 917\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateless_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    918\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    919\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    920\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    921\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3039\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3036\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   3037\u001b[0m   (graph_function,\n\u001b[0;32m   3038\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3039\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   3040\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1963\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1959\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1960\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1961\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1962\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1963\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1964\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1965\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1966\u001b[0m     args,\n\u001b[0;32m   1967\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1968\u001b[0m     executing_eagerly)\n\u001b[0;32m   1969\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:591\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    590\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 591\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    592\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    593\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    594\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    595\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    596\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    597\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    598\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    599\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    600\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    603\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    604\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:59\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     58\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 59\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     60\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     62\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "stop_callback = EarlyStopping(monitor='val_accuracy',patience=5)\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "  if epoch < 10:\n",
    "    return lr\n",
    "  else:\n",
    "    return lr * tf.math.exp(-0.1)\n",
    "\n",
    "lr_callbacks = LearningRateScheduler(scheduler)\n",
    "\n",
    "model.fit(x=X_train,y=y_train,\n",
    "          batch_size=128,\n",
    "          epochs=100,\n",
    "          validation_data=(X_test,y_test),\n",
    "          callbacks=[stop_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6983"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(prediction.argmax(-1) == y_test.argmax(-1)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "78ddfc3686b8b7161f2836984651df038ec9a0366954334fc42499f59ad2b3c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
